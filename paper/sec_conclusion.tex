%\vspace*{-2px}
\section{Conclusion}
\label{sec:conclusion}

We propose a combination of \textbf{robust quantization}, \textbf{weight clipping} and \textbf{random bit error training (\Random)} to get DNNs robustness against random bit errors in their (quantized) weights, enabling low-voltage operation of DNN accelerators to save energy. Here, the accelerator memory is operated far below its rated voltage \cite{ChandramoorthyHPCA2019,KoppulaMICRO2019,KimDATE2018}, inducing exponentially increasing rates of bit errors, directly affecting stored DNN weights. Weight clipping regularizes the weights to a small $[-\wmax, \wmax]$ during training, encouraging redundancy and increasing robustness. \Random further generalizes across chips, with different bit error patterns, and voltages without requiring expensive memory profiling or hardware mitigation strategies. These are important criteria for low-voltage operation in practice. Besides, we also discuss the impact of fixed-point quantization schemes on robustness, which has been neglected in prior work. We are able to train low-precision DNNs robust to significant rates of random bit errors which allow a reduction in energy consumption of roughly $20\%$ or more on \MNIST and \Cifar.
 
\section*{Acknowledgements}
%
MH acknowledges support from the German Federal Ministry of Education and Research
(BMBF) through the Tübingen AI Center (FKZ: 01IS18039A) and from the Deutsche Forschungsgemeinschaft
(DFG, German Research Foundation) under Germany’s Excellence Strategy (EXC number
2064/1, project number 390727645). 
NC acknowledges that this research was developed in part with funding from the U.S. Defense Advanced Research Projects Agency (DARPA). The views, opinions and/or other findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government. DISTRIBUTION STATEMENT A. Approved for public release: distribution unlimited.